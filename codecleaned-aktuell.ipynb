{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the standard packages will be installed and we check which version of the CLMM code is currently installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: import clmm\n",
    "except:\n",
    "    import notebook_install\n",
    "    notebook_install.install_clmm_pipeline(upgrade=False)\n",
    "    import clmm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy\n",
    "from astropy import units\n",
    "import clmm.dataops as da\n",
    "import clmm.galaxycluster as gc\n",
    "import clmm.theory as theory\n",
    "from clmm import Cosmology\n",
    "from clmm import support\n",
    "from clmm.support import mock_data as mock\n",
    "from clmm.support import sampler\n",
    "from clmm.support.sampler import *\n",
    "from clmm.support.sampler import fitters\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "plt.rcParams['font.family']=['gothambook','gotham','gotham-book','serif']\n",
    "\n",
    "clmm.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with the random modul we use random seed in the next step for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create mock data, we need to define a true cosmology, which is currently done with astropy's cosmology library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = Cosmology(H0 = 70.0, Omega_dm0 = 0.27 - 0.045, Omega_b0 = 0.045, Omega_k0 = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moo = clmm.Modeling(massdef = 'critical', delta_mdef = 200, halo_profile_model = 'nfw')\n",
    "moo.set_cosmo(cosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set some parameters for a mock galaxy cluster:\n",
    "    cluster_data[k][0] = cluster_m\n",
    "    cluster_data[k][1] = concentration\n",
    "    cluster_data[k][2] = cluster_ra\n",
    "    cluster_data[k][3] = cluster_dec\n",
    "    cluster_data[k][4] = cluster_z\n",
    "    cluster_data[k][5] = ngals\n",
    "    cluster_data[k][6] = logm\n",
    "    cluster_data[k][7] = number density \n",
    "and print the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff7544c6760>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#could be function\n",
    "\n",
    "lower_limit =13\n",
    "upper_limit =15\n",
    "nbinhist=100\n",
    "norm = (upper_limit-lower_limit)/nbinhist\n",
    "\n",
    "M_star = 2.e14\n",
    "b=4*10**(-5)\n",
    "numsamp= 10**6 #input\n",
    "numstep = 10**4\n",
    "\n",
    "#define x range 1e12-1e15 in num steps\n",
    "x = np.logspace(lower_limit,upper_limit,numstep)\n",
    "#define y values from the function \n",
    "y = (b*(1/(x/M_star))*np.exp(-(x)/M_star))\n",
    "\n",
    "#calculate cumulative sum of the y values (numpy)\n",
    "y_cm = np.cumsum(y)\n",
    "y_cm = y_cm-min(y_cm)\n",
    "y_cm = y_cm/np.max(y_cm)\n",
    "\n",
    "#invert x and y variables (flipping)and interpolate that function \n",
    "f = interp1d(y_cm,x , fill_value=(0,1))\n",
    "#Generate N random numbers uniformly between 0 & 1: u_i~U(0,1)\n",
    "ynew = np.random.random(numsamp)\n",
    "#Using the Inverse of the CDF and the values u_i, compute x_i = F^-1(u_i)\n",
    "x_samp = f(ynew) #return\n",
    "\n",
    "#plot it to see the result\n",
    "plt.hist(x_samp, bins=np.logspace(lower_limit,upper_limit,nbinhist),weights=np.repeat(numsamp/sum(y*x),numsamp)/norm)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.plot(x, y, 'o')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#use that interpolated function to predict num samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)   \n",
    "num = 100\n",
    "cluster_data = np.zeros((num,7))\n",
    "#m_star=1.8.e14\n",
    "for k in range(num):#replace with vector\n",
    "   #cluster_data[k][7] = np.random.randint(1.e(-8),1.e(-2))    \n",
    "    cluster_data[k][0] = x_samp[k] #masse direkt aus der funktion machen\n",
    "    cluster_data[k][6] = np.log(cluster_data[k][0])/np.log(10)#logm\n",
    "    cluster_data[k][2] = 0#ra\n",
    "    cluster_data[k][3] = 0#dec\n",
    "    cluster_data[k][4] = np.random.uniform(0.1,0.6)#redshift\n",
    "    cluster_data[k][1] = 5.72/((1+cluster_data[k][4])**0.71)*(cluster_data[k][0]/10.e14)**(-0.081)#consentration\n",
    "    cluster_data[k][5] = np.random.randint(50,10000)#ngals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_photoz_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicomasii/opt/anaconda3/envs/clmm/lib/python3.9/site-packages/clmm-0.9.3-py3.9.egg/clmm/theory/func_layer.py:347: UserWarning: Some source redshifts are lower than the cluster redshift. kappa = 0 for those galaxies.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m photoz_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mlen\u001b[39m(noisy_data), \u001b[38;5;28mlen\u001b[39m(pzbins_constant)])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mI added this line since the photoz pdf is not computed on a constant redshift grid. simps integral can be optimized using the same x-axis for multiple integrals.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m=================================================================================================================================================================\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mnoisy_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)):\n\u001b[1;32m     60\u001b[0m     photoz_matrix[f,:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minterp(pzbins_constant, noisy_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpzbins\u001b[39m\u001b[38;5;124m'\u001b[39m][f], noisy_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpzpdf\u001b[39m\u001b[38;5;124m'\u001b[39m][f])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m=================================================================================================================================================================\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "ideal_data = []\n",
    "noisy_data = []\n",
    "\n",
    "for k in range(num):\n",
    "    buggy_data = mock.generate_galaxy_catalog(cluster_data[k][0], \n",
    "                                              cluster_data[k][4], \n",
    "                                              cluster_data[k][1], \n",
    "                                                           cosmo, \n",
    "                                                       'chang13',\n",
    "                                                    Delta_SO=200, \n",
    "                                              massdef='critical',\n",
    "                                        halo_profile_model='nfw',\n",
    "                             zsrc_min = cluster_data[k][4] + 0.1,\n",
    "                                                     zsrc_max=3.,\n",
    "                                                 shapenoise=0.05, \n",
    "                                      photoz_sigma_unscaled=0.05, \n",
    "                                    ngals=int(cluster_data[k][5])) \n",
    "   \n",
    "    #sortiert nachher die galaxien raus die den falschen redshift besitzen\n",
    "    mask = buggy_data['z'] < cluster_data[k][4] \n",
    "    #die galaxien sollten entfernt werden nicht maskiert...nur vorrübergehend\n",
    "   \n",
    "    buggy_data['z'] = np.where(buggy_data['z'] < cluster_data[k][4], \n",
    "                               np.random.uniform(cluster_data[k][4],\n",
    "                                            cluster_data[k][4]+0.1), \n",
    "                                                    buggy_data['z'])\n",
    "    \n",
    "    mask = buggy_data['z'] < cluster_data[k][4]\n",
    "    realredshift = buggy_data['z']\n",
    "    #print(sum(buggy_data['z']<=cluster_data[k][4]))\n",
    "    \n",
    "    ideal_data.append(mock.generate_galaxy_catalog(cluster_data[k][0],\n",
    "                                                   cluster_data[k][4], \n",
    "                                                   cluster_data[k][1],\n",
    "                                                                cosmo,\n",
    "                                                            'chang13',\n",
    "                                                         Delta_SO=200, \n",
    "                                                   massdef='critical',\n",
    "                                             halo_profile_model='nfw',\n",
    "                                  zsrc_min = cluster_data[k][4] + 0.1,\n",
    "                                                          zsrc_max=3.,\n",
    "                                     ngals = int(cluster_data[k][5]))) \n",
    "    noisy_data.append(buggy_data)\n",
    "    #abhier gucken was abgeeeeeht\n",
    "    if add_photoz_weights == True:\n",
    "        \n",
    "        pzbins_constant = np.linspace(0,3,30)\n",
    "        \n",
    "        sigma_crit_1 = 1./cosmo.eval_sigma_crit(cluster_data[k][4], pzbins_constant)\n",
    "\n",
    "        photoz_matrix = np.zeros([len(noisy_data), len(pzbins_constant)])\n",
    "        \n",
    "        r\"\"\"\n",
    "        I added this line since the photoz pdf is not computed on a constant redshift grid. simps integral can be optimized using the same x-axis for multiple integrals.\n",
    "        =================================================================================================================================================================\n",
    "        \"\"\"\n",
    "\n",
    "        for f in range(len(noisy_data['id'])):\n",
    "\n",
    "            photoz_matrix[f,:] = np.interp(pzbins_constant, noisy_data['pzbins'][f], noisy_data['pzpdf'][f])\n",
    "            \n",
    "        r\"\"\"\n",
    "        =================================================================================================================================================================\n",
    "        \"\"\"\n",
    "\n",
    "        unormed_integral = scipy.integrate.simps(photoz_matrix * sigma_crit_1, x = pzbins_constant, axis = 1)\n",
    "\n",
    "        norm = scipy.integrate.simps(photoz_matrix, x = pzbins_constant, axis = 1)\n",
    "\n",
    "        noisy_data['sigma_c_photoz'] = (unormed_integral/norm)**(-1)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((ideal_data[0][:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compete the radius which we will need later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "radius = []\n",
    "for k in range(num):\n",
    "    variable = clmm.dataops._compute_lensing_angles_astropy(cluster_data[k][2], \n",
    "                                                            cluster_data[k][3], \n",
    "                                                            ideal_data[k]['ra'], \n",
    "                                                            ideal_data[k]['dec'])\n",
    "    radius.append(variable) \n",
    "    \n",
    "    print(radius)\n",
    "    #numpy arrays instead(cluster_data macht das so) inline for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxy catalogs are converted to a clmm.GalaxyCluster object and may be saved for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_object =[]\n",
    "for k in range(num):\n",
    "\n",
    "    cluster_id = \"CL_ideal\"\n",
    "    gc_object.append( clmm.GalaxyCluster(cluster_id, \n",
    "                                 cluster_data[k][2],\n",
    "                                 cluster_data[k][3],\n",
    "                                 cluster_data[k][4], \n",
    "                                     ideal_data[k]))\n",
    "    gc_object[k].save('ideal_GC_'+str(k)+'.pkl')\n",
    "\n",
    "gc_object =[]\n",
    "\n",
    "for k in range(num):\n",
    "\n",
    "    cluster_id = \"CL_noisy\"\n",
    "    gc_object.append( clmm.GalaxyCluster(cluster_id,\n",
    "                                 cluster_data[k][2],\n",
    "                                 cluster_data[k][3],\n",
    "                                 cluster_data[k][4], \n",
    "                                     noisy_data[k]))\n",
    "    gc_object[k].save('noisy_GC_'+str(k)+'.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gc_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any saved clmm.GalaxyCluster object may be read in for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_ideal = []\n",
    "cl_noisy = []\n",
    "for k in range(num):\n",
    "    cl_ideal.append (clmm.GalaxyCluster.load('ideal_GC_'+str(k)+'.pkl'))\n",
    "    cl_noisy.append (clmm.GalaxyCluster.load('noisy_GC_'+str(k)+'.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redshift of galaxies generated by mock data are distributed following the Chang. (2013) redshift distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving observables\n",
    "\n",
    "Computing shear\n",
    "\n",
    "clmm.GalaxyCluster.compute_tangential_and_cross_components calculates the tangential and cross shears for each source galaxy in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num):\n",
    "    cl_ideal[k].compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "    cl_noisy[k].compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radially binning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = da.make_bins(0.2, 4, 15, method='evenwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num):\n",
    "\n",
    "    a = cl_ideal[k].make_radial_profile(\"Mpc\",\n",
    "                      include_empty_bins=True, \n",
    "                               bins=bin_edges, \n",
    "                                  cosmo=cosmo, \n",
    "                         gal_ids_in_bins=True,)#return_binnumber=True does not work\n",
    "    \n",
    "    b = cl_noisy[k].make_radial_profile(\"Mpc\",\n",
    "                      include_empty_bins=True, \n",
    "                               bins=bin_edges,\n",
    "                                  cosmo=cosmo, \n",
    "                         gal_ids_in_bins=True,)\n",
    "\n",
    "    #masking\n",
    "    maski = cl_ideal[k].profile['z'] > cluster_data[k][4]\n",
    "    maskn = cl_noisy[k].profile['z'] > cluster_data[k][4]\n",
    "    mask= maski*maskn\n",
    "    cl_ideal[k].profile =  cl_ideal[k].profile[mask]\n",
    "    cl_noisy[k].profile =  cl_noisy[k].profile[mask]\n",
    " \n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the object acquires the clmm.GalaxyCluster.profile attribute.\n",
    "\n",
    "#Create the reduced tangential shear models\n",
    "We consider two options:\n",
    "\n",
    "First, the naive and wrong approach: the reduced tangential shear in a given radial bin 𝑗 is given by 𝑔𝑡(𝜃𝑗,⟨𝑧𝑠⟩), where ⟨𝑧𝑠⟩ is the average redshift in the bin. In that case, the corresponding model is simply given by the fucntion below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_reduced_tangential_shear_zdistrib(radius, \n",
    "                                              logm,\n",
    "                                              data,\n",
    "                                           catalog,\n",
    "                                           profile, \n",
    "                                         cluster_z,\n",
    "                                    concentration): \n",
    "    m = 10**logm\n",
    "    gt_model = []\n",
    "    for i in range(len(radius)):\n",
    "        \n",
    "        r = profile['radius'][i]\n",
    "        galist = profile['gal_id'][i]\n",
    "        \n",
    "        z_list = catalog.galcat['z'][galist]\n",
    "        shear = clmm.compute_reduced_tangential_shear(r,\n",
    "                                                      m,\n",
    "                                          concentration,\n",
    "                                              cluster_z,\n",
    "                                                 z_list, \n",
    "                                                  cosmo, \n",
    "                                         delta_mdef=200, \n",
    "                               halo_profile_model='nfw')\n",
    "        if len(galist) == 0:\n",
    "            gt_model.append(1e-16)\n",
    "            print(\"this is bad\")\n",
    "        else:\n",
    "            gt_model.append(np.mean(shear))\n",
    "\n",
    "    return gt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_reduced_tangential_shear_singlez(r,\n",
    "                                        logm,\n",
    "                                       z_src,\n",
    "                                   cluster_z,\n",
    "                              concentration):\n",
    "    m = 10.**logm\n",
    "    gt_model = clmm.compute_reduced_tangential_shear(r,\n",
    "                                                     m,\n",
    "                                         concentration,\n",
    "                                             cluster_z,\n",
    "                                                 z_src,\n",
    "                                                 cosmo,\n",
    "                                        delta_mdef=200,\n",
    "                              halo_profile_model='nfw')    \n",
    "    return gt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting, let's first vizualise these models using the known true mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_model_ideal_singlez = []\n",
    "gt_model_ideal_zdistrib = []\n",
    "gt_model_noisy_singlez = []\n",
    "gt_model_noisy_zdistrib = []\n",
    "r=[]\n",
    "for k in range(num):\n",
    "    r.append(cl_ideal[k].profile['radius'])\n",
    "\n",
    "    gt_model_ideal_singlez.append(model_reduced_tangential_shear_singlez(r[k], \n",
    "                                                           cluster_data[k][6],\n",
    "                                                     cl_ideal[k].profile['z'],\n",
    "                                                           cluster_data[k][4], \n",
    "                                                          cluster_data[k][1]))\n",
    "    \n",
    "    gt_model_ideal_zdistrib.append(model_reduced_tangential_shear_zdistrib(r[k],\n",
    "                                                             cluster_data[k][6],\n",
    "                                                                  ideal_data[k],\n",
    "                                                                    cl_ideal[k], \n",
    "                                                            cl_ideal[k].profile,\n",
    "                                                             cluster_data[k][4], \n",
    "                                                            cluster_data[k][1]))\n",
    "    \n",
    "    gt_model_noisy_singlez.append(model_reduced_tangential_shear_singlez(r[k],\n",
    "                                                           cluster_data[k][6],\n",
    "                                                     cl_noisy[k].profile['z'],\n",
    "                                                           cluster_data[k][4], \n",
    "                                                          cluster_data[k][1]))\n",
    "    \n",
    "    gt_model_noisy_zdistrib.append(model_reduced_tangential_shear_zdistrib(r[k],\n",
    "                                                             cluster_data[k][6],\n",
    "                                                                  noisy_data[k], \n",
    "                                                                    cl_noisy[k], \n",
    "                                                            cl_noisy[k].profile,\n",
    "                                                             cluster_data[k][4], \n",
    "                                                            cluster_data[k][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive model that uses the average redshift in the bin clearly does not give the right description of the ideal data (left panel), and will yield biased mass results if used for fitting (see below). For ideal data, the model that accounts for the redshift distribution is, by construction, an excellent description of the data (solid blue line). The same is true for noisy data (right panel), although the noise make the naive model appear \"less biased\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mass fitting\n",
    "We estimate the best-fit mass using scipy.optimize.curve_fit. We compare estimated mass for noisy and ideal data, using both models described above (naive with average redshift or the model taking into account the redshift distribution). The choice of fitting log10𝑀 instead of 𝑀 lowers the range of pre-defined fitting bounds from several order of magnitude for the mass to unity. From the associated error Δ(log10𝑀) we calculate the error to mass as Δ𝑀=𝑀𝑓𝑖𝑡log(10)Δ(log10𝑀)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num):\n",
    "\n",
    "    for l in range(np.size(cl_ideal[k].profile)):\n",
    "        if cl_ideal[k].profile['gt_err'][l] == 0:\n",
    "            cl_ideal[k].profile['gt_err'][l] = 0.0001\n",
    "            #das sollte ich definitiv noch ändern geschummelt ich haben\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est_ideal_zdistrib = [None]*num\n",
    "m_est_err_ideal_zdistrib = [None]*num\n",
    "for k in range(num):\n",
    "\n",
    "    func = lambda r, x : model_reduced_tangential_shear_zdistrib(r,x, \n",
    "                                                       ideal_data[k],\n",
    "                                                         cl_ideal[k], \n",
    "                                                 cl_ideal[k].profile, \n",
    "                                                  cluster_data[k][4], \n",
    "                                                  cluster_data[k][1])\n",
    "\n",
    "    popt,pcov = fitters['curve_fit'](func, \n",
    "                        cl_ideal[k].profile['radius'], \n",
    "                        cl_ideal[k].profile['gt'], \n",
    "                        cl_ideal[k].profile['gt_err'], bounds=[10.,17.])#p0 war bei 14.6 habe ich gelöscht\n",
    "\n",
    "    \n",
    "    m_est_ideal_zdistrib[k] = 10.**popt[0]\n",
    "    m_est_err_ideal_zdistrib[k] =  m_est_ideal_zdistrib[k] * np.sqrt(pcov[0][0]) * np.log(10) \n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est_ideal_singlez = [None]*num\n",
    "m_est_err_ideal_singlez = [None]*num\n",
    "for k in range(num):\n",
    "\n",
    "    funct = lambda r, logm : model_reduced_tangential_shear_singlez(r,\n",
    "                                                                 logm, \n",
    "                                             cl_ideal[k].profile['z'], \n",
    "                                                   cluster_data[k][4], \n",
    "                                                   cluster_data[k][1])\n",
    "    popt,pcov = fitters['curve_fit'](funct, \n",
    "                        cl_ideal[k].profile['radius'], \n",
    "                        cl_ideal[k].profile['gt'], \n",
    "                        cl_ideal[k].profile['gt_err'], bounds=[10.,17.])\n",
    "\n",
    "    m_est_ideal_singlez[k] = 10.**popt[0]\n",
    "    m_est_err_ideal_singlez[k] = m_est_ideal_singlez[k] * np.sqrt(pcov[0][0]) * np.log(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num):\n",
    "\n",
    "    for l in range(np.size(cl_ideal[k].profile)):\n",
    "        if cl_noisy[k].profile['gt_err'][l] == 0:\n",
    "            cl_noisy[k].profile['gt_err'][l] = 0.0022713490203193337\n",
    "            #das sollte ich definitiv noch ändern geschummelt ich haben\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est_noisy_zdistrib = [None]*num\n",
    "m_est_err_noisy_zdistrib = [None]*num\n",
    "for k in range(num):\n",
    "    cluster_z = cluster_data[k][4]\n",
    "    concentration = cluster_data[k][1]\n",
    "    #logm = cluster_data[k][6]\n",
    "    funct = lambda r, logm : model_reduced_tangential_shear_zdistrib(r, \n",
    "                                                                  logm, \n",
    "                                                         noisy_data[k],\n",
    "                                                           cl_noisy[k], \n",
    "                                                   cl_noisy[k].profile, \n",
    "                                                    cluster_data[k][4], \n",
    "                                                    cluster_data[k][1]) \n",
    "    popt,pcov = fitters['curve_fit'](funct, \n",
    "                        cl_noisy[k].profile['radius'], \n",
    "                        cl_noisy[k].profile['gt'], \n",
    "                        cl_noisy[k].profile['gt_err'], bounds=[10.,16.])\n",
    "\n",
    "    m_est_noisy_zdistrib[k] = 10.**popt[0]\n",
    "    m_est_err_noisy_zdistrib[k] =  m_est_noisy_zdistrib[k] * np.sqrt(pcov[0][0]) * np.log(10) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est_noisy_singlez = [None]*num\n",
    "m_est_err_noisy_singlez =[None]*num\n",
    "for k in range(num):\n",
    "\n",
    "    funct = lambda r, logm : model_reduced_tangential_shear_singlez(r, \n",
    "                                                                 logm, \n",
    "                                             cl_noisy[k].profile['z'],\n",
    "                                                   cluster_data[k][4], \n",
    "                                                   cluster_data[k][1])\n",
    "    popt,pcov = fitters['curve_fit'](funct, \n",
    "                        cl_noisy[k].profile['radius'], \n",
    "                        cl_noisy[k].profile['gt'], \n",
    "                        cl_noisy[k].profile['gt_err'],bounds=[10.,16.])\n",
    "\n",
    "    m_est_noisy_singlez[k] = 10**popt[0]\n",
    "    m_est_err_noisy_singlez[k] =  m_est_noisy_singlez[k] * np.sqrt(pcov[0][0])*np.log(10)\n",
    "    m_est_err_noisy_singlez_lg = np.sqrt(pcov[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the reconstructed mass is biased when the redshift distribution is not accounted for in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the results\n",
    "For visualization purpose, we calculate the reduced tangential shear predicted by the model with estimated masses for noisy and ideal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_est_ideal_zdistrib = []\n",
    "gt_est_noisy_zdistrib = []\n",
    "gt_est_ideal_singlez = []\n",
    "gt_est_noisy_singlez = []\n",
    "\n",
    "for k in range(num):\n",
    "\n",
    "    gt_est_ideal_zdistrib.append(model_reduced_tangential_shear_zdistrib\n",
    "                                                                     (r[k],\n",
    "                                np.log(m_est_ideal_zdistrib[k])/np.log(10), \n",
    "                                                             ideal_data[k], \n",
    "                                                               cl_ideal[k], \n",
    "                                                       cl_ideal[k].profile, \n",
    "                                                        cluster_data[k][4], \n",
    "                                                       cluster_data[k][1]))\n",
    "\n",
    "    gt_est_noisy_zdistrib.append(model_reduced_tangential_shear_zdistrib\n",
    "                                                                     (r[k], \n",
    "                                np.log(m_est_noisy_zdistrib[k])/np.log(10), \n",
    "                                                             noisy_data[k], \n",
    "                                                               cl_noisy[k], \n",
    "                                                       cl_noisy[k].profile,\n",
    "                                                        cluster_data[k][4], \n",
    "                                                       cluster_data[k][1]))\n",
    "\n",
    "    gt_est_ideal_singlez.append(model_reduced_tangential_shear_singlez\n",
    "                                                                    (r[k], \n",
    "                                np.log(m_est_ideal_singlez[k])/np.log(10),\n",
    "                                                 cl_ideal[k].profile['z'],\n",
    "                                                       cluster_data[k][4], \n",
    "                                                      cluster_data[k][1]))\n",
    "    \n",
    "    gt_est_noisy_singlez.append(model_reduced_tangential_shear_singlez\n",
    "                                                                    (r[k],\n",
    "                                np.log(m_est_noisy_singlez[k])/np.log(10),\n",
    "                                                 cl_noisy[k].profile['z'],\n",
    "                                                       cluster_data[k][4],\n",
    "                                                      cluster_data[k][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare to tangential shear obtained with theoretical mass. We plot the reduced tangential shear models first when redshift distribution is accounted for in the model then for the naive approach, with respective best-fit masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the average of the input masses\n",
    "cluster_data_mass_avg = np.average(cluster_data[:,0])\n",
    "#calculate the avere of the estimated mass (ideal data) (bad method)\n",
    "m_est_ideal_singlez_avg = np.average(m_est_ideal_singlez)\n",
    "#calculate the standarddeviation of the estimated mass (ideal data) (bad method)\n",
    "m_est_ideal_singlez_avg_err = np.sum(m_est_err_ideal_singlez)/np.size(m_est_err_ideal_singlez)\n",
    "\n",
    "\n",
    "#calculate the avere of the estimated mass (noisy data) (bad method)\n",
    "m_est_noisy_singlez_avg = np.average(m_est_noisy_singlez)\n",
    "#calculate the standarddeviation of the estimated mass (noisy data) (bad method)\n",
    "m_est_noisy_singlez_avg_err = np.sum(m_est_err_noisy_singlez)/np.size(m_est_err_noisy_singlez)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#calculate the avere of the estimated mass (ideal data)(good method)\n",
    "m_est_ideal_zdistrib_avg = np.average(m_est_ideal_zdistrib)\n",
    "#calculate the standarddeviation of the estimated mass (ideal data)(good method)\n",
    "m_est_ideal_zdistrib_avg_err = np.sum(m_est_err_ideal_zdistrib)/np.size(m_est_err_ideal_zdistrib)\n",
    "\n",
    "\n",
    "#calculate the avere of the estimated mass (noisy data)(good method)\n",
    "m_est_noisy_zdistrib_avg = np.average(m_est_noisy_zdistrib)\n",
    "#calculate the standarddeviation of the estimated mass (noisy data)(good method)\n",
    "m_est_noisy_zdistrib_avg_err = np.sum(m_est_err_noisy_zdistrib)/np.size(m_est_err_noisy_zdistrib)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input estimated masszdistrib\n",
    "plt.hist(m_est_ideal_singlez, bins=np.logspace(13,15,100), facecolor='y', alpha=0.5, label=\"estimated mass ideal\")\n",
    "plt.hist(m_est_noisy_singlez, bins=np.logspace(13,15,100), facecolor='b', alpha=0.5, label=\"estimated mass noisy\")\n",
    "plt.axvline(cluster_data_mass_avg, color='k', linestyle='solid', linewidth=1 , label=\"input mass average\")\n",
    "plt.axvline(m_est_ideal_singlez_avg, color='c', linestyle='solid', linewidth=1, label=\"estimated mass avg ideal\")\n",
    "plt.axvline(m_est_ideal_singlez_avg-m_est_ideal_singlez_avg_err, color='c', linestyle='dashed', linewidth=1, label=\"estimated mass avg err ideal\")\n",
    "plt.axvline(m_est_ideal_singlez_avg+m_est_ideal_singlez_avg_err, color='c', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(m_est_noisy_singlez_avg, color='r', linestyle='solid', linewidth=1, label=\"estimated mass avg noisy\")\n",
    "plt.axvline(m_est_noisy_singlez_avg-m_est_noisy_singlez_avg_err, color='r', linestyle='dashed', linewidth=1, label=\"estimated mass avg err noisy\")\n",
    "plt.axvline(m_est_noisy_singlez_avg+m_est_noisy_singlez_avg_err, color='r', linestyle='dashed', linewidth=1)\n",
    "plt.hist(cluster_data[:,0],bins=np.logspace(13,15,100), facecolor='g', alpha=0.5 , label=\"input\")\n",
    "#plt.errorbar(m_est_ideal_singlez_avg,0, xerr=m_est_ideal_singlez_avg_err, fmt=\"\", color=\"r\")\n",
    "plt.xlabel('Estimated mass vs input mass bad method')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.ylabel('N')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input estimated masszdistrib\n",
    "pyrand = np.random.normal(m_est_ideal_zdistrib\n",
    "                        ,m_est_err_ideal_zdistrib)\n",
    "#plt.plot(pyrand)\n",
    "plt.hist(m_est_ideal_zdistrib, bins=np.logspace(13,15,100),  facecolor='y', alpha=0.5, label=\"estimated mass ideal\")\n",
    "plt.hist(m_est_noisy_zdistrib, bins=np.logspace(13,15,100),  facecolor='b', alpha=0.5, label=\"estimated mass noisy\")\n",
    "plt.axvline(cluster_data_mass_avg, color='k', linestyle='solid', linewidth=1 , label=\"input mass average\")\n",
    "plt.axvline(m_est_ideal_zdistrib_avg, color='c',alpha=0.5, linestyle='solid', linewidth=1, label=\"estimated mass avg ideal\")\n",
    "plt.axvline(m_est_ideal_zdistrib_avg-m_est_ideal_zdistrib_avg_err, color='c', linestyle='dashed', linewidth=1, label=\"estimated mass avg err ideal\")\n",
    "plt.axvline(m_est_ideal_zdistrib_avg+m_est_ideal_zdistrib_avg_err, color='c', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(m_est_noisy_zdistrib_avg, color='r', linestyle='solid', linewidth=1, label=\"estimated mass avg noisy\")\n",
    "plt.axvline(m_est_noisy_zdistrib_avg-m_est_noisy_zdistrib_avg_err, color='r', linestyle='dashed', linewidth=1, label=\"estimated mass avg err noisy\")\n",
    "plt.axvline(m_est_noisy_zdistrib_avg+m_est_noisy_zdistrib_avg_err, color='r', linestyle='dashed', linewidth=1)\n",
    "plt.hist(cluster_data[:,0], bins=np.logspace(13,15,100), facecolor='g', alpha=0.5 , label=\"input\")\n",
    "#plt.errorbar(m_est_ideal_singlez_avg,0, xerr=m_est_ideal_singlez_avg_err, fmt=\"\", color=\"r\")\n",
    "plt.xlabel('Estimated mass vs input mass good method')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel('N')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "difference_ideal_singlez = np.log10(m_est_ideal_singlez)-np.log10(cluster_data[:,0])\n",
    "#print(difference)\n",
    "std_ideal_singlez = (1/np.log(10))*np.array(m_est_err_ideal_singlez)/np.array(m_est_ideal_singlez)\n",
    "#print(mean)\n",
    "#print(std)\n",
    "x_min=-5\n",
    "x_max=5\n",
    "\n",
    "x = np.linspace(x_min, x_max, 1000)\n",
    "ysum_ideal_singlez = 0\n",
    "\n",
    "for k in range(num):\n",
    "\n",
    "    y_ideal_singlez = scipy.stats.norm.pdf(x,difference_ideal_singlez[k],std_ideal_singlez[k])\n",
    "    #print(y[k])\n",
    "    #print(mean[k])\n",
    "    #print(std[k])\n",
    "    ysum_ideal_singlez = ysum_ideal_singlez + y_ideal_singlez\n",
    "    \n",
    "    #plt.plot(x,y, color='coral')\n",
    "\n",
    "   \n",
    "\n",
    "    plt.plot(x,y_ideal_singlez, color='coral',alpha=0.5)\n",
    "#print(ysum)\n",
    "\n",
    "plt.plot(x,ysum_ideal_singlez/200, color='blue')\n",
    "plt.xlabel('$\\Delta$M')\n",
    "plt.ylabel('N')\n",
    "\n",
    "#plt.plot(y,z)\n",
    "\n",
    "plt.show()\n",
    "#print(mean)\n",
    "#print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "difference_noisy_singlez = np.log10(m_est_noisy_singlez)-np.log10(cluster_data[:,0])\n",
    "#print(difference)\n",
    "std_noisy_singlez = (1/np.log(10))*np.array(m_est_err_noisy_singlez)/np.array(m_est_noisy_singlez)\n",
    "#print(mean)\n",
    "#print(std)\n",
    "x_min=-5\n",
    "x_max=5\n",
    "\n",
    "x = np.linspace(x_min, x_max, 1000)\n",
    "ysum_noisy_singlez = 0\n",
    "\n",
    "for k in range(num):\n",
    "\n",
    "    y_noisy_singlez = scipy.stats.norm.pdf(x,difference_noisy_singlez[k],std_noisy_singlez[k])\n",
    "    #print(y[k])\n",
    "    #print(mean[k])\n",
    "    #print(std[k])\n",
    "    ysum_noisy_singlez = ysum_noisy_singlez + y_noisy_singlez\n",
    "    \n",
    "    #plt.plot(x,y, color='coral')\n",
    "\n",
    "   \n",
    "\n",
    "    plt.plot(x,y_noisy_singlez, color='coral',alpha=0.5)\n",
    "#print(ysum)\n",
    "\n",
    "plt.plot(x,ysum_noisy_singlez/200, color='blue')\n",
    "plt.xlabel('$\\Delta$M')\n",
    "plt.ylabel('N')\n",
    "\n",
    "#plt.plot(y,z)\n",
    "\n",
    "plt.show()\n",
    "#print(mean)\n",
    "#print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "difference_ideal_zdistrib = np.log10(m_est_ideal_zdistrib)-np.log10(cluster_data[:,0])\n",
    "#print(difference)\n",
    "std_ideal_zdistrib = (1/np.log(10))*np.array(m_est_err_ideal_zdistrib)/np.array(m_est_ideal_zdistrib)\n",
    "#print(mean)\n",
    "#print(std)\n",
    "x_min=-5\n",
    "x_max=5\n",
    "\n",
    "x = np.linspace(x_min, x_max, 1000)\n",
    "ysum_ideal_zdistrib = 0\n",
    "\n",
    "for k in range(num):\n",
    "\n",
    "    y_ideal_zdistrib = scipy.stats.norm.pdf(x,difference_ideal_zdistrib[k],std_ideal_zdistrib[k])\n",
    "    #print(y[k])\n",
    "    #print(mean[k])\n",
    "    #print(std[k])\n",
    "    ysum_ideal_zdistrib = ysum_ideal_zdistrib + y_ideal_zdistrib\n",
    "    \n",
    "    #plt.plot(x,y, color='coral')\n",
    "\n",
    "   \n",
    "\n",
    "    plt.plot(x,y_ideal_zdistrib, color='coral',alpha=0.5)\n",
    "#print(ysum)\n",
    "\n",
    "plt.plot(x,ysum_ideal_zdistrib/200, color='blue')\n",
    "plt.xlabel('$\\Delta$M')\n",
    "plt.ylabel('N')\n",
    "\n",
    "#plt.plot(y,z)\n",
    "\n",
    "plt.show()\n",
    "#print(mean)\n",
    "#print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from math import log, exp\n",
    "difference_noisy_zdistrib = np.log10(m_est_noisy_zdistrib)-np.log10(cluster_data[:,0])\n",
    "#print(difference)\n",
    "std_noisy_zdistrib = (1/np.log(10))*np.array(m_est_err_noisy_zdistrib)/np.array(m_est_noisy_zdistrib)\n",
    "#print(mean)\n",
    "#print(std)\n",
    "x_min=-5\n",
    "x_max=5\n",
    "\n",
    "x = np.linspace(x_min, x_max, 1000)\n",
    "ysum_noisy_zdistrib = 0\n",
    "\n",
    "for k in range(num):\n",
    "\n",
    "    y_noisy_zdistrib = scipy.stats.norm.pdf(x,difference_noisy_zdistrib[k],std_noisy_zdistrib[k])\n",
    "    ysum_noisy_zdistrib = ysum_noisy_zdistrib + y_noisy_zdistrib\n",
    "    \n",
    "    #plt.plot(x,y, color='coral')\n",
    "\n",
    "   \n",
    "\n",
    "    plt.plot(x,y_noisy_zdistrib, color='coral',alpha=0.5)\n",
    "#print(ysum)\n",
    "\n",
    "plt.plot(x,ysum_noisy_zdistrib/200, color='blue')\n",
    "plt.xlabel('$\\Delta$M')\n",
    "plt.ylabel('N')\n",
    "\n",
    "#plt.plot(y,z)\n",
    "\n",
    "plt.show()\n",
    "#print(mean)\n",
    "#print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alle methoden in einem Plot\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].scatter(np.log10(cluster_data[:,0]),difference_ideal_singlez, alpha = 0.5, s=0.1, marker=',')\n",
    "axs[0, 0].set_title('ideal_singlez')\n",
    "axs[0, 0].errorbar(np.log10(cluster_data[:,0]), difference_ideal_singlez, yerr=std_ideal_singlez, fmt='.', ecolor = 'orange',c='black')\n",
    "axs[0, 0].set_xlim([12.75,16])\n",
    "axs[0, 0].set_ylim([-0.08,0.08])\n",
    "\n",
    "\n",
    "axs[1, 0].scatter(np.log10(cluster_data[:,0]),difference_noisy_singlez, alpha = 0.5, s=0.1, marker=',')\n",
    "axs[1, 0].set_title('noisy_singlez')\n",
    "axs[1, 0].errorbar(np.log10(cluster_data[:,0]), difference_noisy_singlez, yerr=std_noisy_singlez, fmt='.', ecolor = 'orange',c='black')\n",
    "axs[1, 0].set_xlim([12.75,16])\n",
    "axs[1, 0].set_ylim([-2,2])\n",
    "\n",
    "axs[0, 1].scatter(np.log10(cluster_data[:,0]),difference_ideal_zdistrib, alpha = 0.5, s=0.1, marker=',')\n",
    "axs[0, 1].set_title('ideal_zdistrib')\n",
    "axs[0, 1].errorbar(np.log10(cluster_data[:,0]), difference_ideal_zdistrib, yerr=std_ideal_zdistrib, fmt='.', ecolor = 'orange',c='black')\n",
    "axs[0, 1].set_xlim([12.75,16])\n",
    "axs[0, 1].set_ylim([-0.08,0.08])\n",
    "\n",
    "axs[1, 1].scatter(np.log10(cluster_data[:,0]),difference_noisy_zdistrib, alpha = 0.5, s=0.1, marker=',')\n",
    "axs[1, 1].set_title('noisy_zdistrib')\n",
    "axs[1, 1].errorbar(np.log10(cluster_data[:,0]), difference_noisy_zdistrib, yerr=std_noisy_zdistrib, fmt='.', ecolor = 'orange',c='black')\n",
    "axs[1, 1].set_xlim([12.75,16])\n",
    "axs[1, 1].set_ylim([-2,2])\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='log(M_true)', ylabel='log10($\\Delta$M)')\n",
    "\n",
    " #Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alle methoden in einem Plot\n",
    "ysum_ideal_singlez = 0\n",
    "ysum_noisy_singlez = 0\n",
    "ysum_ideal_zdistrib = 0\n",
    "ysum_noisy_zdistrib = 0\n",
    "fig, axs = plt.subplots(2,1)\n",
    "\n",
    "for k in range(num):\n",
    "    y_noisy_singlez = scipy.stats.norm.pdf(x,difference_noisy_singlez[k],std_noisy_singlez[k])\n",
    "    ysum_noisy_singlez = ysum_noisy_singlez + y_noisy_singlez\n",
    "    axs[0].plot(x,y_noisy_singlez, color='coral',alpha=0.5)\n",
    "\n",
    "axs[0].plot(x,ysum_noisy_singlez/100, color='blue')\n",
    "axs[0].set_title('noisy_singlez')\n",
    "axs[0].set_xlim([-4,4])\n",
    "axs[0].set_ylim([-0.5,10])\n",
    "\n",
    "\n",
    "for k in range(num):\n",
    "    y_noisy_zdistrib = scipy.stats.norm.pdf(x,difference_noisy_zdistrib[k],std_noisy_zdistrib[k])\n",
    "    ysum_noisy_zdistrib = ysum_noisy_zdistrib + y_noisy_zdistrib\n",
    "    axs[1].plot(x,y_noisy_zdistrib, color='coral',alpha=0.5)\n",
    "    \n",
    "axs[1].plot(x,ysum_noisy_zdistrib/100, color='blue')\n",
    "axs[1].set_title('noisy_zdistrib')\n",
    "axs[1].set_xlim([-4,4])\n",
    "axs[1].set_ylim([-0.5,10])\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='$\\Delta$M', ylabel='N')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compute the excess surface density\n",
    "import clmm.theory as m\n",
    "mass_Delta = 200\n",
    "DeltaSigma = []\n",
    "r3d = []\n",
    "density_profile_parametrization = 'nfw'\n",
    "for k in range(num):\n",
    "    r3d = np.logspace(-2, 2, 15)\n",
    "    r3d = np.linspace(0.2,4,15)\n",
    "    cluster_mass = cluster_data[k][0]\n",
    "    cluster_concentration = cluster_data[k][1]\n",
    "    z_cl = cluster_data[k][4]\n",
    "    DeltaSigma.append(m.compute_excess_surface_density(r3d, cluster_mass, cluster_concentration, z_cl, cosmo=cosmo, \n",
    "                                              delta_mdef=mass_Delta, \n",
    "                                              halo_profile_model=density_profile_parametrization))\n",
    "print(np.shape(DeltaSigma[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the excess Surface density against R\n",
    "# Label für die y-Achse vergeben:\n",
    "plt.ylabel('$\\Delta\\Sigma$')\n",
    "plt.xlabel('R3d-Radial position from the cluster center')\n",
    "# Einen x-y-Plot erstellen:\n",
    "plt.plot(r3d, DeltaSigma[0],color='black', linestyle='', marker='.')\n",
    "\n",
    "# Achsen-Bereiche manuell festlegen\n",
    "# Syntax: plt.axis([xmin, xmax, ymin, ymax])\n",
    "#plt.axis([0, 5, 0, 20])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# Ein gepunktetes Diagramm-Gitter einblenden:\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Diagramm anzeigen:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "DeltaSigmastack = np.zeros(15)\n",
    "DeltaSigmaarray = np.array(DeltaSigma)\n",
    "print(type(DeltaSigmastack))\n",
    "for k in range(num):\n",
    "    DeltaSigmastack = DeltaSigmastack + DeltaSigmaarray[k]\n",
    "print(DeltaSigmastack/num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the excess Surface density against R\n",
    "# Label für die y-Achse vergeben:\n",
    "plt.ylabel('$\\Delta\\Sigma-stacked average$')\n",
    "plt.xlabel('R3d-Radial position from the cluster center')\n",
    "# Einen x-y-Plot erstellen:\n",
    "plt.plot(r3d, DeltaSigmastack/num,color='black', linestyle='', marker='.')\n",
    "\n",
    "# Achsen-Bereiche manuell festlegen\n",
    "# Syntax: plt.axis([xmin, xmax, ymin, ymax])\n",
    "#plt.axis([0, 5, 0, 20])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# Ein gepunktetes Diagramm-Gitter einblenden:\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Diagramm anzeigen:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(cl_ideal[0].profile['z'])\n",
    "print(k)\n",
    "print(cluster_data[0][4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval sigma_crit from gamma t\n",
    "#delta_sigma= gamma_t * sigma_crit\n",
    "import clmm.cosmology.ccl as tt\n",
    "cosm=tt.CCLCosmology()\n",
    "Sigmacritideal=[]\n",
    "Sigmacritnoisy=[]\n",
    "for k in range(num): \n",
    "    Sigmacritideal.append(cosm.eval_sigma_crit(z_src=cl_ideal[k].profile['z'], z_len=cluster_data[k][4]))\n",
    "    Sigmacritnoisy.append(cosm.eval_sigma_crit(z_src=cl_noisy[k].profile['z'], z_len=cluster_data[k][4]))\n",
    "#delta sigma from gamma t ideal:\n",
    "deltasigma_gt_ideal=cl_ideal[0].profile['gt']*Sigmacritideal[0]\n",
    "#delta sigma from gamma t noisy:\n",
    "deltasigma_gt_noisy=cl_noisy[0].profile['gt']*Sigmacritnoisy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3d1 = np.logspace(0.2, 4, 15)\n",
    "r3d1 = cl_ideal[0].profile['radius']\n",
    "#Plot the excess Surface density against R\n",
    "# Label für die y-Achse vergeben:\n",
    "plt.ylabel('$\\Delta\\Sigma$')\n",
    "plt.xlabel('R3d-Radial position from the cluster center')\n",
    "# Einen x-y-Plot erstellen:\n",
    "plt.plot(r3d1, deltasigma_gt_ideal,color='black', linestyle='', marker='.', label=\"$\\Delta\\Sigma(\\gamma t)-ideal$\")\n",
    "plt.plot(r3d1, deltasigma_gt_noisy,color='blue', linestyle='', marker='.', label=\"$\\Delta\\Sigma(\\gamma t)-noisy$\")\n",
    "plt.plot(r3d, DeltaSigma[0],color='orange', linestyle='', marker='.', label=\"$\\Delta\\Sigma from input mass$\")\n",
    "# Achsen-Bereiche manuell festlegen\n",
    "# Syntax: plt.axis([xmin, xmax, ymin, ymax])\n",
    "#plt.axis([0, 5, 0, 20])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# Ein gepunktetes Diagramm-Gitter einblenden:\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Diagramm anzeigen:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import clmm\n",
    "import clmm.dataops\n",
    "from clmm.dataops import compute_tangential_and_cross_components, make_radial_profile, make_bins\n",
    "from clmm.galaxycluster import GalaxyCluster\n",
    "import clmm.utils as u\n",
    "from clmm import Cosmology\n",
    "from clmm.support import mock_data as mock\n",
    "cosmo = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)\n",
    "import numpy as np\n",
    "cosmo_clmm = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from numpy import random\n",
    "import chainconsumer\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_surface_density(single_catalog = None, radial_bin = None, sigma_c = None, radius = None):\n",
    "    r\"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    \n",
    "    single_catalog : GalaxyCluster object\n",
    "    \n",
    "    radial_bin : liste\n",
    "        radial bins to evaluate the binned excess surface density\n",
    "    \n",
    "    sigma_c : string \n",
    "        column name in the single_catalog for the critical surface mass density\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    ds, r, sum_weights : array, array, array\n",
    "        the binned excess surface density, the binned radius, the sum of weights w_ls in each radial bin\n",
    "\n",
    "    \"\"\"\n",
    "    #gammat radial bins\n",
    "    #single_catalog.galcat['R']=radius\n",
    "    #sigma_c = sigma_crit\n",
    "    #single_catalog.galcat['et']=gt\n",
    "    ds = np.zeros(len(radial_bin))\n",
    "    rad = np.zeros(len(radial_bin))\n",
    "    sum_weights = np.zeros(len(radial_bin))\n",
    "    \n",
    "    radius=single_catalog.profile['radius']\n",
    "    \n",
    "    for i, r_bin in enumerate(radial_bin):\n",
    "        mask = (radius > r_bin[0])*(radius < r_bin[1])\n",
    "        if any(mask) : \n",
    "            w_ls = 1./np.array(sigma_c)[mask]**2\n",
    "            ds[i] = np.average(np.array(single_catalog.profile['gt'][mask]*sigma_c[mask]), weights = w_ls)\n",
    "            #\n",
    "            rad[i] = np.average(radius[mask])\n",
    "            sum_weights[i] = np.sum( w_ls )\n",
    "    return ds, rad, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from clmm.galaxycluster import GalaxyCluster\n",
    "new_bins = np.linspace(0.2, 4, 15)\n",
    "radial_bin = [[new_bins[s],new_bins[s+1]] for s in range(len(new_bins)-1)]\n",
    "\n",
    "\n",
    "\n",
    "names = ['ds_single', 'r_single', 'W_l','c', 'z', 'mass'] #liste\n",
    "print(type(names))\n",
    "\n",
    "Stack_file_true = {name : [] for name in names} #dictonary\n",
    "print(type(Stack_file_true))\n",
    " \n",
    "Stack_file_photoz = {name : [] for name in names} #dictonary\n",
    "\n",
    "n_catalogs = num\n",
    "for i in range(n_catalogs):\n",
    "    ds_single, r_single, sum_weights_single = excess_surface_density(single_catalog = cl_ideal[i], radial_bin = radial_bin, sigma_c = Sigmacritideal[i]) #tuple unpacking\n",
    "    #funktion gibt 3 sachen zurück\n",
    "    data_to_store = [ds_single, r_single, sum_weights_single, cluster_data[i][1], cluster_data[i][4], cluster_data[i][0]]\n",
    "    #speichern der daten\n",
    "    for d, name in enumerate(names):\n",
    "        Stack_file_true[name].append(data_to_store[d])\n",
    "    \n",
    "    ds_single, r_single, sum_weights_single = excess_surface_density(single_catalog = cl_noisy[i], radial_bin = radial_bin, sigma_c = Sigmacritnoisy[i])\n",
    "    data_to_store = [ds_single, r_single, sum_weights_single, cluster_data[i][1], cluster_data[i][4], cluster_data[i][0]]\n",
    "    for d, name in enumerate(names):\n",
    "        Stack_file_photoz[name].append(data_to_store[d])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm import utils\n",
    "meanx,meany,yerr,num_objects,binnumber=utils.compute_radial_averages(Stack_file_photoz['r_single'][0],Stack_file_photoz['ds_single'][0],xbins=bin_edges,error_model='std')\n",
    "print(yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Stack_file_true)\n",
    "print(Stack_file_photoz)\n",
    "print(type(ds_single))\n",
    "print(type(cluster_data[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3d1 = np.logspace(-2, 2, 15)\n",
    "#Plot the excess Surface density against R\n",
    "# Label für die y-Achse vergeben:\n",
    "plt.ylabel('$\\Delta\\Sigma$')\n",
    "plt.xlabel('R3d-Radial position from the cluster center')\n",
    "# Einen x-y-Plot erstellen:\n",
    "plt.plot(Stack_file_true['r_single'], Stack_file_true['ds_single'],color='black', linestyle='', marker='.')\n",
    "plt.plot(Stack_file_photoz['r_single'], Stack_file_photoz['ds_single'],color='blue', linestyle='', marker='.')\n",
    "plt.plot(r3d, DeltaSigma[0],color='orange', linestyle='', marker='.')\n",
    "# Achsen-Bereiche manuell festlegen\n",
    "# Syntax: plt.axis([xmin, xmax, ymin, ymax])\n",
    "#plt.axis([0, 5, 0, 20])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# Ein gepunktetes Diagramm-Gitter einblenden:\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Diagramm anzeigen:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked excess surface density profile - true redshift case\n",
    "print(Stack_file_true['W_l'])\n",
    "ds_mean_true = np.average(Stack_file_true['ds_single'], weights = Stack_file_true['W_l'], axis = 0)\n",
    "r_mean_true = np.average(Stack_file_true['r_single'], weights = None, axis = 0)\n",
    "\n",
    "#stacked excess surface density profile - photoz redshift case\n",
    "ds_mean_photoz = np.average(Stack_file_photoz['ds_single'], weights = Stack_file_photoz['W_l'], axis = 0)\n",
    "r_mean_photoz = np.average(Stack_file_photoz['r_single'], weights = None, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm import utils\n",
    "meanx,meany,yerr,num_objects,binnumber=utils.compute_radial_averages(r_mean_photoz,ds_mean_photoz,bin_edges,error_model='std')\n",
    "print(yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r3d1 = np.logspace(-2, 2, 15)\n",
    "#Plot the excess Surface density against R\n",
    "# Label für die y-Achse vergeben:\n",
    "plt.ylabel('$\\Delta\\Sigma$')\n",
    "plt.xlabel('R3d-Radial position from the cluster center')\n",
    "# Einen x-y-Plot erstellen:\n",
    "plt.plot(r_mean_true, ds_mean_true,color='black', linestyle='', marker='.', label=\"$\\Delta\\Sigma(\\gamma t) stacked average-truez$\")\n",
    "plt.plot(r_mean_photoz, ds_mean_photoz,color='blue', linestyle='', marker='.', label=\"$\\Delta\\Sigma(\\gamma t)stacked average-photoz$\")\n",
    "plt.plot(r3d, DeltaSigma[0],color='orange', linestyle='', marker='.', label=\"$\\Delta\\Sigma from input mass$\")\n",
    "# Achsen-Bereiche manuell festlegen\n",
    "# Syntax: plt.axis([xmin, xmax, ymin, ymax])\n",
    "#plt.axis([0, 5, 0, 20])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "# Ein gepunktetes Diagramm-Gitter einblenden:\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Diagramm anzeigen:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(corner): return [[corner[i],corner[i+1]] for i in range(len(corner)-1)]\n",
    "\n",
    "def jackknife(single_catalog = None, N_jk = None):\n",
    "    \n",
    "    r\"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    single_catalog : GalaxyCluster object\n",
    "        GalaxyCluster object of a single cluster \n",
    "    N_jk : int\n",
    "        number of jackknife regions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cov : array\n",
    "        Jackknife delete-1 covariance matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    phi = np.linspace(-np.pi, np.pi, N_jk + 1)\n",
    "    \n",
    "    phi_bin = binning(phi)\n",
    "    \n",
    "    gt = []\n",
    "    \n",
    "    mask_list_in = [(phi_[0] < single_catalog.profile['phi'])*(single_catalog.profile['phi'] < phi_[1]) for phi_ in phi_bin]\n",
    "    \n",
    "    for s, phi_ in enumerate(phi_bin):\n",
    "        \n",
    "        mask_in = mask_list_in[s]\n",
    "        \n",
    "        mask_out = np.invert(mask_in)\n",
    "        \n",
    "        plt.scatter(single_catalog.profile['ra'][mask_in], single_catalog.profile['dec'][mask_in], s = 1)\n",
    "        \n",
    "        data_cut = single_catalog.profile[mask_out]\n",
    "        \n",
    "        cl_cut_jack = clmm.GalaxyCluster('Stack', single_catalog.ra, single_catalog.dec, single_catalog.z, single_catalog.galcat[mask_out])\n",
    "        \n",
    "        ds, r, sum_weights = excess_surface_density(single_catalog = cl_cut_jack, radial_bin = radial_bin, sigma_c = 'sigma_c')\n",
    "        \n",
    "        gt.append(ds)\n",
    "\n",
    "    gt = np.array(gt)\n",
    "    \n",
    "    cov = np.cov(gt.T)*(N_jk-1)**2/N_jk\n",
    "    \n",
    "    plt.xlabel('ra')\n",
    "    plt.ylabel('dec')\n",
    "    plt.axis('equal')\n",
    "        \n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_single = jackknife(single_catalog = cl_ideal, N_jk = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
